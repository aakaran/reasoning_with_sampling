<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reasoning Without Training: Your Base Model is Smarter Than You Think - achieving RL-level reasoning performance through inference-time MCMC sampling.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reasoning Without Training: Your Base Model is Smarter Than You Think</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <style>
    .publication-title {
      font-family: 'Google Sans', sans-serif;
    }
    .publication-authors {
      font-family: 'Google Sans', sans-serif;
    }
    .dnerf {
      font-weight: bold;
      color: #3273dc;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Reasoning Without Training: Your Base Model is Smarter Than You Think</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Aayush Karan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Yilun Du</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harvard University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
<div class="is-size-5 mt-3">
                <span class="has-text-weight-bold">Summary:</span> With our sampler, base models can achieve single-shot reasoning performance on par with RL without any additional training or access to a verifier while avoiding a collapse in generation diversity and multi-shot (pass@k) performance.
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="placeholder-teaser.png" alt="Teaser figure showing performance comparison" style="width: 100%;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, 
            driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, 
            despite the widespread success of this paradigm, much of the literature has been devoted to 
            disentangling truly novel behaviors that emerge during RL but are not present in the base models.
          </p>
          <p>
            In our work, we approach this question from a different angle, instead asking whether comparable 
            reasoning capabilities can be elicited from base models at inference time, <strong>without any 
            additional training</strong>. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling 
            from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base 
            models' own likelihoods.
          </p>
          <p>
            Over different base models, we show that our algorithm offers substantial boosts in reasoning that 
            nearly match and even outperform those from RL on a wide variety of single-shot tasks, including 
            MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple 
            samples that is characteristic of RL posttraining. Crucially, our method does not require training, curated 
            datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h3 class="title is-4">Power Distributions for Reasoning</h3>
        <div class="content has-text-justified">
          <p>
            RL has emerged as the central paradigm to enhance reasoning capabilities in frontier models,
            leading to substantial boosts in performance across domains such as mathematics and coding. At the same time,
            there is evidence to suggest that the reasoning chains that emerge after RL-posttraining are well within base model
            capabilities. For example, we can plot the log likelihoods and confidences (i.e., negative average per-token entropies)
            of outputs <strong>with respect to the base model distribution</strong> and observe that RL outputs tightly concentrate around 
            high likelihood and high confidence regions of the base model. This points towards an effective <strong>distribution sharpening</strong>,
            where RL shifts probability mass from low-likelihood sequences to high-likelihood ones. 
          </p>
          <div style="text-align:center; margin: 24px 0;">
            <img src="combined_hists.png" alt="Combined Histograms" width="90%">
          </div>
          <p>
            Motivated by this observation, we introduce sampling from the <strong>power distribution</strong> \( p^{\alpha} \), 
            which naturally sharpens the base model distribution \(p\) by upweighting high-likelihood sequences via exponentiation. 
            Crucially, unlike simple low-temperature sampling, power distributions account for future completion 
            likelihoods, favoring tokens with fewer but higher likelihood future paths. This is especially valuable for reasoning tasks, 
            as it encourages avoiding "critical windows" or "pivotal tokens" that trap outputs in low-likelihood futures.
          </p>
        </div>

        <h3 class="title is-4">Autoregressive MCMC Sampling</h3>
        <div class="content has-text-justified">
          <p>
            Directly sampling from \( p^{\alpha} \) is intractable, as it requires normalizing over a sequence space that 
            is exponential in length. To get around this, we employ a Metropolis-Hastings (MCMC) method to approximately 
            sample from the unnormalized distribution \( p^{\alpha} \). Metropolis-Hastings iteratively updates a sample
            generation \(\mathbf{x}\) by proposing a new candidate \(\mathbf{x'}\) and accepting the change with some probability
            \(A(\mathbf{x}, \mathbf{x'})\) which depends on the \( p^{\alpha} \) weights. We illustrate the process below:


            In general, Metropolis-Hastings can require exponentially many iterative updates before converging to sampling from 
            \( p^{\alpha} \) with such a large sequence space. To avoid this curse of dimensionality, we build the output block-by-block,
            using Metropolis-Hastings to sample from \( p^{\alpha} \) for progressively longer sequences. This amounts to probabilistic iterative 
            resampling informed by base model likelihoods. 
          </p>
          <!-- Embed block START -->
          <figure style="margin: 24px 0; text-align: center;">
            <div style="
                max-width: 900px;
                margin: 0 auto;
                border: 1px solid #e5e7eb;
                border-radius: 10px;
                overflow: hidden;
                aspect-ratio: 50 / 9;">
              <!-- Use iframe for an external HTML file -->
              <iframe
                src="block_mcmc.html"
                title="Metropolisâ€“Hastings Illustration"
                style="width: 100%; height: 100%; border: 0;"
                loading="lazy"
                referrerpolicy="no-referrer"
                allowfullscreen>
              </iframe>
            </div>
            <figcaption style="color:#6b7280; font-size: 0.9rem; margin-top: 8px;">
              Illustration of block-wise autoregressive MCMC sampling.
            </figcaption>
            <noscript>
              Your browser has JavaScript disabled. You can <a href="mh_animation.html">open the interactive version</a>.
            </noscript>
          </figure>
          <!-- Embed block END -->
          <p>
            Our algorithm is thus <strong>training-free</strong>, <strong>dataset-free</strong>, and 
            <strong>verifier-free</strong>, avoiding the hyperparameter tuning, dataset curation, and 
            reward signal requirements of RL posttraining
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    
    <div class="content has-text-justified">
      <p>
        We benchmark our Power Sampling algorithm across three base models (Qwen2.5-Math-7B, Qwen2.5-7B, 
        Phi-3.5-mini-instruct) on four tasks: MATH500, HumanEval, GPQA, and AlpacaEval 2.0.
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Main Performance Results</h3>
        <div class="content">
          <p><strong>Key Findings:</strong></p>
          <ul>
            <li><strong>On in-domain tasks (MATH500)</strong>: Power Sampling achieves accuracies comparable to GRPO, 
            with improvements up to +25.2% over base models</li>
            <li><strong>On out-of-domain tasks (HumanEval)</strong>: Our method often outperforms GRPO, with gains 
            up to +51.9% over base models</li>
            <li><strong>On unverifiable tasks (AlpacaEval 2.0)</strong>: Power Sampling consistently outperforms 
            GRPO, demonstrating generalizability beyond verifiable domains</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Diversity and Pass@k Performance</h3>
        <div class="content has-text-justified">
          <p>
            Unlike GRPO, which exhibits collapsed diversity in multi-shot sampling, our Power Sampling 
            maintains generation diversity. On pass@k metrics (where a problem is solved if at least one 
            of k samples is correct), our method:
          </p>
          <ul>
            <li>Strongly outperforms GRPO for k > 1</li>
            <li>Supersedes base model performance until converging at high k</li>
            <li>Achieves GRPO-level single-shot performance without compromising multi-shot capability</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Emergent Properties</h3>
        <div class="content has-text-justified">
          <p>
            Our analysis reveals several interesting characteristics of Power Sampling:
          </p>
          <ul>
            <li><strong>Higher base model likelihoods</strong>: Samples concentrate in high-likelihood regions 
            while maintaining diversity</li>
            <li><strong>Higher confidence</strong>: Responses exhibit similar confidence levels to GRPO samples</li>
            <li><strong>Long-form reasoning</strong>: Average response length increases naturally (from 600 to 
            679 tokens on MATH500) without explicit encouragement</li>
            <li><strong>Distinct reasoning traces</strong>: Qualitative analysis shows Power Sampling finds 
            different solution paths than GRPO</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Algorithm Overview</h2>

        <div class="content has-text-justified">
          <p>
            Our Power Sampling algorithm works by:
          </p>
          <ol>
            <li><strong>Defining intermediate distributions</strong>: We progressively sample from distributions 
            over increasing sequence lengths (block size B)</li>
            <li><strong>Initializing with proposals</strong>: Each new block starts with tokens sampled from a 
            proposal distribution (typically low-temperature base model)</li>
            <li><strong>MCMC refinement</strong>: For N<sub>MCMC</sub> steps, we randomly resample subsequences 
            and accept/reject based on the power distribution likelihood ratio</li>
            <li><strong>Building up sequences</strong>: This process continues until reaching the target length, 
            progressively approaching samples from p<sup>Î±</sup></li>
          </ol>
          <p>
            The algorithm requires only O(N<sub>MCMC</sub>TÂ²/4B) token generations on average, making it 
            practically viable for inference-time scaling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implications</h2>

        <div class="content has-text-justified">
          <p>
            Our work demonstrates that <strong>base models are significantly more capable at reasoning than 
            standard sampling methods reveal</strong>. The success of Power Sampling suggests:
          </p>
          <ul>
            <li>High-likelihood regions of base models strongly correlate with correct reasoning</li>
            <li>Additional inference-time compute can unlock latent reasoning capabilities</li>
            <li>RL posttraining may primarily perform distribution sharpening rather than learning fundamentally 
            new behaviors</li>
            <li>Training-free methods can extend reasoning beyond easily verifiable domains where RL struggles</li>
          </ul>
          <p>
            This opens promising directions for enhancing reasoning capabilities through better understanding and 
            utilization of base model distributions, rather than relying solely on expensive posttraining procedures.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{karan2025reasoning,
  author    = {Karan, Aayush and Du, Yilun},
  title     = {Reasoning Without Training: Your Base Model is Smarter Than You Think},
  journal   = {NeurIPS},
  year      = {2025},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is borrowed from <a href="https://nerfies.github.io/">Nerfies</a>, 
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
